---
title: "Data Manipulation and Visualization"
author: "Instructors: Daniel Falster & Will Cornwell"
date: "17/12/2018"
output: html_document
---

```{r}
library(tidyverse)
data_beaches <- read_csv("data/sydney_beaches/sydneybeaches.csv")
```

# Introduction to data manipulation with `dplyr` intro

(Will, 11:15 - 12:15)

## Motivation

Data is never organized in the way you want it 

High % of project is data wrangling

Many many many modern jobs are data wrangling

## Key ideas

Data is a noun

Actions on data are verbs  

Learn a few key verbs and what they do

- filter

- select

- mutate

- arrange

- distinct

- group_by

- summarise

- n

- rename

- recode

- count

Pipes


- use the operator `%>%` 
- If you use RStudio, you can type the pipe with Ctrl + Shift + M if you have a PC or Cmd + Shift + M if you have a Mac.

the `join` family

**Exercises:** Apply the `dplyr` package and your new data wrangling skills to the sydney beaches dataset to 

1. Filter the Beach dataset to only Latitudes that are north of -33.94
2. Mutate a row to be in usable dates using lubridate::dmy()
3. select only Region, Site, Latitude, and Longitude columns and then use distinct to get a date frame of only locations
4. recode the Council column values to not include the word "Council" (it's redundant)
5. Count the number of records by Council; count the number of records by Site AND Council

**Resources:**

- [Data manipulation on environmentalcomputing.net](http://environmentalcomputing.net/data-manipulation/) 
- [Datacarpentry's lesson on data manipulation](https://datacarpentry.org/R-ecology-lesson/03-dplyr.html)
- [The `tidylog` package](https://github.com/elbersb/tidylog) is cool for understanding what happens with dplyr operations

# Lunch

(12:30-13:30)


# Data Wrangling

(13:30-14:00)

**Exercises:** Apply the `dplyr` package and your new data wrangling skills to the Sydney beaches dataset to 

1. Create a subset of the dataset consisting of only records taken since 2016
2. As above and only variables `Site`, `Latitude`, and `Longitude`
3. As above but with only one record per Site
3. As above but with data sorted by `Latitude`
4. For each Site, calculate the number of records, the min, mean, median and max value of `Enterococci_cfu_per_100ml` 
5. For each record, create a new variable giving the year the record was collected (hint: the package lubridate could be helpful, see the cheatsheet) 

**Extra Exercises** 

1. Join the Sydney beaches data to the rain data in "data/sydney_beaches/randwickrain.csv" by date using left_join, inner_join, full_join and right_join.  Try to understand what each of those does.  
